{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_Optiver Realized Volatility Prediction",
      "provenance": [],
      "mount_file_id": "1dy1gsy4VvNGaDjO-jDeOyw1z3Wtdq8ap",
      "authorship_tag": "ABX9TyNENxcE+QU4cM0ThI8vTaFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbbwoy/CodeStudy/blob/main/Kaggle_Optiver_Realized_Volatility_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/leolu1998/lgbm-tabnet-nn-no-leaks-stratifiedgroupkfold/notebook "
      ],
      "metadata": {
        "id": "Ps72ZFKTk1p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**: stock market data relevant to the practical execution of trades in the financial markets\n",
        "- order book snapshots and executed trades\n",
        "\n",
        "- 150,000 target values\n"
      ],
      "metadata": {
        "id": "Q5Rvl3fumHc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Files**\n",
        "\n",
        "book_[train/test]\n",
        "- 가장 자주 매매되는 주문에 대한 order book data\n",
        "- level 1: more competitive in price terms, receive execution priority over the second level.\n",
        "\n",
        "1. stock_id - ID code for the stock. 결측치 존재, Parquet 로딩 시 범주형으로 강제 변환하므로 int8로 변환할 것.\n",
        "2. time_id - ID code for the time bucket. 순차적이진 않지만 모든 주식에서 일관성.\n",
        "3. seconds_in_bucket - Number of seconds from the start of the bucket, 항상 0부터 시작\n",
        "4. bid_price[1/2] - 높은 매도를 기록한 주식의 정규화된 가격\n",
        "5. ask_price[1/2] - 높은 매수를 기록한 주식의 정규화된 가격\n",
        "6. bid_size[1/2] - 높은 매도를 기록한 주식의 정규화된 수\n",
        "7. ask_size[1/2] - 높은 매수를 기록한 주식의 정규화된 수\n",
        "\n",
        "trade_[train/test].\n",
        "- 실거래 데이터 \n",
        "\n",
        "1. stock_id - Same as above.\n",
        "2. time_id - Same as above.\n",
        "3. seconds_in_bucket - Same as above. 반드시 0에서 시작X\n",
        "5. price - 1초 동안 발생한 거래의 평균 가격(정규화). 평균은 각 거래에서 거래된 주식 수로 가중치 부여\n",
        "6. size - 거래 주식의 총수\n",
        "7. order_count - 발생한 고유 거래 주문 수\n",
        "\n"
      ],
      "metadata": {
        "id": "D0HFtsHniCc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train.csv \n",
        "\n",
        "1. stock_id - Same as above. 범주형X 정수로 로드.\n",
        "2. time_id - Same as above.\n",
        "3. target - 동일한 주식/시간ID의 데이터를 따라 10분에 걸쳐 계산된 실현 변동성. 피처 데이터와 타겟 데이터 사이 중복X. \n",
        "\n"
      ],
      "metadata": {
        "id": "2EcVYlrAmBKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test.csv \n",
        "\n",
        "1. stock_id - Same as above.\n",
        "2. time_id - Same as above.\n",
        "3. row_id - 제출 행의 고유 ID. 각 기존 시간/재고ID에는 하나의 행이 있음. 각 시간 창에는 모든 개별 주식이 포함되지 않음."
      ],
      "metadata": {
        "id": "CX7vYp8YmaW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tabnet"
      ],
      "metadata": {
        "id": "QapDYIeHn8q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet==3.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwnFOI96EgqN",
        "outputId": "bac7b96f-c728-4e6a-f95d-842fc26b997a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-tabnet==3.1.1\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.0.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (4.64.0)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.12.1+cu113)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet==3.1.1) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (4.1.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy.matlib\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import norm\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "import torch\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts"
      ],
      "metadata": {
        "id": "3eANRUxNGbLd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting some global config\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "orange_black = ['#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820']\n",
        "plt.rcParams['figure.figsize'] = (16, 9)\n",
        "plt.rcParams[\"figure.facecolor\"] = '#FFFACD'\n",
        "plt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "plt.rcParams[\"grid.color\"] = orange_black[3]\n",
        "plt.rcParams[\"grid.alpha\"] = 0.5\n",
        "plt.rcParams[\"grid.linestyle\"] = '--'\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uRt3pKfBo7xh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "def read_train_test():\n",
        "    #Function to read our base train and test set\n",
        "    train = pd.read_csv('/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/train.csv')\n",
        "    test = pd.read_csv('/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/test.csv')\n",
        "\n",
        "    #Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    print(f'Our test set has {test.shape[0]} rows')\n",
        "    print(f'Our training set has {train.isna().sum().sum()} missing values')\n",
        "    print(f'Our test set has {test.isna().sum().sum()} missing values')\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "train, test = read_train_test()"
      ],
      "metadata": {
        "id": "cQGFJ23gpPRn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97c0fc70-1b01-4025-a3e3-910f7d054e0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our training set has 428932 rows\n",
            "Our test set has 3 rows\n",
            "Our training set has 0 missing values\n",
            "Our test set has 0 missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "Gn_hsk8XqpY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0b317051-e18b-40ab-8e7c-4d3855c06d93"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   stock_id  time_id    target row_id\n",
              "0         0        5  0.004136    0-5\n",
              "1         0       11  0.001445   0-11\n",
              "2         0       16  0.002168   0-16\n",
              "3         0       31  0.002195   0-31\n",
              "4         0       62  0.001747   0-62"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97d82ff3-d788-4574-9153-7249efc26214\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock_id</th>\n",
              "      <th>time_id</th>\n",
              "      <th>target</th>\n",
              "      <th>row_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.004136</td>\n",
              "      <td>0-5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>0-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0-62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97d82ff3-d788-4574-9153-7249efc26214')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97d82ff3-d788-4574-9153-7249efc26214 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97d82ff3-d788-4574-9153-7249efc26214');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "qYl_sg4GEUDA",
        "outputId": "7835ed0b-e9a9-4d9a-b852-3f25eed618a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   stock_id  time_id row_id\n",
              "0         0        4    0-4\n",
              "1         0       32   0-32\n",
              "2         0       34   0-34"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42400a4e-bea5-4c84-bb2a-97b28fa27110\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stock_id</th>\n",
              "      <th>time_id</th>\n",
              "      <th>row_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0-32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>34</td>\n",
              "      <td>0-34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42400a4e-bea5-4c84-bb2a-97b28fa27110')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42400a4e-bea5-4c84-bb2a-97b28fa27110 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42400a4e-bea5-4c84-bb2a-97b28fa27110');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data directory\n",
        "data_dir = \"/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/\"\n",
        "\n",
        "def calc_wap1(df):\n",
        "    #wap1 계산\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "def realized_volatility(series):\n",
        "    # Calculate the realized volatility\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "def count_unique(series):\n",
        "    # Function to count unique elements of a series\n",
        "    return len(np.unique(series))"
      ],
      "metadata": {
        "id": "De_XUKkpp87z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def book_preprocessor(file_path):\n",
        "    # Function to preprocess book data (for each stock id)\n",
        "    \n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    \n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    \n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    \n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.mean, np.std],\n",
        "        'wap2': [np.sum, np.mean, np.std],\n",
        "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'wap_balance': [np.sum, np.mean, np.std],\n",
        "        'price_spread':[np.sum, np.mean, np.std],\n",
        "        'price_spread2':[np.sum, np.mean, np.std],\n",
        "        'bid_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_spread':[np.sum, np.mean, np.std],\n",
        "        'total_volume':[np.sum, np.mean, np.std],\n",
        "        'volume_imbalance':[np.sum, np.mean, np.std],\n",
        "        \"bid_ask_spread\":[np.sum, np.mean, np.std],\n",
        "    }\n",
        "\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Function to get group stats for different windows (seconds in bucket)\n",
        "            \n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "            \n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "            \n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature           #여기 뭐지.....?\n",
        "        \n",
        "        # Get the stats for different windows\n",
        "        df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "        df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
        "        df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "        df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
        "\n",
        "        def tendency(price, vol):    \n",
        "            df_diff = np.diff(price)\n",
        "            val = (df_diff/price[1:])*100\n",
        "            power = np.sum(val*vol[1:])\n",
        "            return(power)\n",
        "    \n",
        "        lis = []\n",
        "        for n_time_id in df['time_id'].unique():\n",
        "            df_id = df[df['time_id'] == n_time_id]        \n",
        "            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
        "            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
        "            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
        "            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
        "            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
        "            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
        "            energy = np.mean(df_id['price'].values**2)\n",
        "            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
        "            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
        "            energy_v = np.sum(df_id['size'].values**2)\n",
        "            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
        "            \n",
        "            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
        "                    'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
        "        \n",
        "        df_lr = pd.DataFrame(lis)\n",
        "            \n",
        "    \n",
        "        df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
        "\n",
        "        # Merge all\n",
        "        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "\n",
        "        # Drop unnecesary time_ids\n",
        "        df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n",
        "        \n",
        "            \n",
        "        # Create row_id so we can merge\n",
        "        stock_id = file_path.split('=')[1]\n",
        "        df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "        df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "        \n",
        "        return df_feature"
      ],
      "metadata": {
        "id": "XhXuwuoiqkfL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trade_preprocessor(file_path):\n",
        "    # Function to preprocess trade data (for each stock id)\n",
        "    \n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n",
        "        'order_count':[np.mean,np.sum,np.max],\n",
        "    }\n",
        "\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Function to get group stats for different windows (seconds in bucket)\n",
        "        \n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        \n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        \n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n",
        "\n",
        "    def tendency(price, vol):    \n",
        "        df_diff = np.diff(price)\n",
        "        val = (df_diff/price[1:])*100\n",
        "        power = np.sum(val*vol[1:])\n",
        "        return(power)\n",
        "    \n",
        "    lis = []\n",
        "    for n_time_id in df['time_id'].unique():\n",
        "        df_id = df[df['time_id'] == n_time_id]        \n",
        "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
        "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
        "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
        "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
        "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
        "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
        "        energy = np.mean(df_id['price'].values**2)\n",
        "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
        "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
        "        energy_v = np.sum(df_id['size'].values**2)\n",
        "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
        "        \n",
        "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
        "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
        "    \n",
        "    df_lr = pd.DataFrame(lis)\n",
        "        \n",
        "   \n",
        "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
        "\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "\n",
        "    def order_sum(df, sec:str):\n",
        "        new_col = 'size_tau' + sec\n",
        "        bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n",
        "        df[new_col] = np.sqrt(1/df[bucket_col])\n",
        "            \n",
        "        new_col2 = 'size_tau2' + sec\n",
        "        order_col = 'trade_order_count_sum' + sec\n",
        "        df[new_col2] = np.sqrt(1/df[order_col])\n",
        "            \n",
        "        if sec == '400_':\n",
        "            df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n",
        "        \n",
        "\n",
        "    \n",
        "    for sec in ['','_200','_300','_400']:\n",
        "        order_sum(df_feature, sec)\n",
        "        \n",
        "    df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n",
        "    \n",
        "    return df_feature"
      ],
      "metadata": {
        "id": "5JJ4iqaHFnOh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_stock(df):\n",
        "    # Function to get group stats for the stock_id and time_id\n",
        "    \n",
        "    # Get realized volatility columns\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    \n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    \n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "uO-bZZ127CLu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_agg_features(train, test):\n",
        "\n",
        "    # Making agg features\n",
        "\n",
        "    train_p = pd.read_csv('/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/train.csv')\n",
        "    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "    corr = train_p.corr()\n",
        "    ids = corr.index\n",
        "    kmeans = KMeans(n_clusters=7, random_state=42).fit(corr.values)\n",
        "    l = []\n",
        "    for n in range(7):\n",
        "        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "\n",
        "    mat = []\n",
        "    matTest = []\n",
        "    n = 0\n",
        "    for ind in l:\n",
        "        newDf = train.loc[train['stock_id'].isin(ind) ]\n",
        "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "        mat.append ( newDf )\n",
        "        newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
        "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "        newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "        matTest.append ( newDf )\n",
        "        n+=1\n",
        "\n",
        "    mat1 = pd.concat(mat).reset_index()\n",
        "    mat1.drop(columns=['target'],inplace=True)\n",
        "    mat2 = pd.concat(matTest).reset_index()\n",
        "    \n",
        "    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
        "    \n",
        "    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "    mat1.reset_index(inplace=True)\n",
        "    \n",
        "    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "    mat2.reset_index(inplace=True)\n",
        "    \n",
        "    prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n",
        "              'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n",
        "    selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n",
        "    selected_cols.append('time_id')\n",
        "    \n",
        "    train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n",
        "    test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n",
        "    \n",
        "    # filling missing values with train means\n",
        "\n",
        "    features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n",
        "    train_m[features] = train_m[features].fillna(train_m[features].mean())\n",
        "    test_m[features] = test_m[features].fillna(train_m[features].mean())\n",
        "\n",
        "    return train_m, test_m"
      ],
      "metadata": {
        "id": "o1KfTEuy7F_s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    # Funtion to make preprocessing function in parallel (for each stock id)\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    \n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get group stats of time_id and stock_id\n",
        "train = get_time_stock(train)\n",
        "test = get_time_stock(test)\n",
        "\n",
        "# Fill inf values\n",
        "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "\n",
        "# Aggregating some features\n",
        "train, test = create_agg_features(train,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 902
        },
        "id": "aGi5_3Tx7JSY",
        "outputId": "beec2434-c21d-4f62-cb47-ad7f3a0a9d1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-17-0cbe0af012c1>\", line 16, in for_joblib\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\", line 119, in merge\n    validate=validate,\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\", line 627, in __init__\n    _left = _validate_operand(left)\n  File \"/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\", line 2283, in _validate_operand\n    f\"Can only merge Series or DataFrame objects, a {type(obj)} was passed\"\nTypeError: Can only merge Series or DataFrame objects, a <class 'NoneType'> was passed\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-0cbe0af012c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Preprocess them using Parallel and our single stock id functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_stock_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'row_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0cbe0af012c1>\u001b[0m in \u001b[0;36mpreprocessor\u001b[0;34m(list_stock_ids, is_train)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Use parallel api to call paralle for loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_joblib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstock_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_stock_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Concatenate all the dataframes that return from Parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Can only merge Series or DataFrame objects, a <class 'NoneType'> was passed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for fold in range(5):\n",
        "    !cp -r ../input/optiver-tabnet-models/fold{str(fold)}/* .\n",
        "    !zip fold{str(fold)}.zip model_params.json network.pt\n",
        "    \n",
        "modelpath = [os.path.join(\"./\",s) for s in os.listdir(\"./\") if (\"zip\" in s)] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDwvEBSk8z5z",
        "outputId": "128cea89-8761-4f6f-f323-21af91a90fac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '../input/optiver-tabnet-models/fold0/*': No such file or directory\n",
            "\tzip warning: name not matched: model_params.json\n",
            "\tzip warning: name not matched: network.pt\n",
            "\n",
            "zip error: Nothing to do! (fold0.zip)\n",
            "cp: cannot stat '../input/optiver-tabnet-models/fold1/*': No such file or directory\n",
            "\tzip warning: name not matched: model_params.json\n",
            "\tzip warning: name not matched: network.pt\n",
            "\n",
            "zip error: Nothing to do! (fold1.zip)\n",
            "cp: cannot stat '../input/optiver-tabnet-models/fold2/*': No such file or directory\n",
            "\tzip warning: name not matched: model_params.json\n",
            "\tzip warning: name not matched: network.pt\n",
            "\n",
            "zip error: Nothing to do! (fold2.zip)\n",
            "cp: cannot stat '../input/optiver-tabnet-models/fold3/*': No such file or directory\n",
            "\tzip warning: name not matched: model_params.json\n",
            "\tzip warning: name not matched: network.pt\n",
            "\n",
            "zip error: Nothing to do! (fold3.zip)\n",
            "cp: cannot stat '../input/optiver-tabnet-models/fold4/*': No such file or directory\n",
            "\tzip warning: name not matched: model_params.json\n",
            "\tzip warning: name not matched: network.pt\n",
            "\n",
            "zip error: Nothing to do! (fold4.zip)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "y = train['target']\n",
        "X_test=test.copy()\n",
        "X_test.drop(['time_id','row_id'], axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "bUJ1ecOI82an"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmspe(y_true, y_pred):\n",
        "    # Function to calculate the root mean squared percentage error\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))"
      ],
      "metadata": {
        "id": "QzzYWIxX83yv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSPE(Metric):\n",
        "    def __init__(self):\n",
        "        self._name = \"rmspe\"\n",
        "        self._maximize = False\n",
        "\n",
        "    def __call__(self, y_true, y_score):\n",
        "        \n",
        "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))"
      ],
      "metadata": {
        "id": "-yedFo5z84UD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RMSPELoss(y_pred, y_true):\n",
        "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()\n",
        "\n",
        "nunique = X.nunique()\n",
        "types = X.dtypes\n",
        "\n",
        "categorical_columns = []\n",
        "categorical_dims =  {}\n",
        "\n",
        "for col in X.columns:\n",
        "    if  col == 'stock_id':\n",
        "        l_enc = LabelEncoder()\n",
        "        X[col] = l_enc.fit_transform(X[col].values)\n",
        "        X_test[col] = l_enc.transform(X_test[col].values)\n",
        "        categorical_columns.append(col)\n",
        "        categorical_dims[col] = len(l_enc.classes_)\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n",
        "        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "eUhh9RJm88dh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
        "\n",
        "cat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n",
        "\n",
        "tabnet_params = dict(\n",
        "    cat_idxs=cat_idxs,\n",
        "    cat_dims=cat_dims,\n",
        "    cat_emb_dim=1,\n",
        "    n_d = 16,\n",
        "    n_a = 16,\n",
        "    n_steps = 2,\n",
        "    gamma = 2,\n",
        "    n_independent = 2,\n",
        "    n_shared = 2,\n",
        "    lambda_sparse = 0,\n",
        "    optimizer_fn = Adam,\n",
        "    optimizer_params = dict(lr = (2e-2)),\n",
        "    mask_type = \"entmax\",\n",
        "    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n",
        "    scheduler_fn = CosineAnnealingWarmRestarts,\n",
        "    seed = 42,\n",
        "    verbose = 10\n",
        "    \n",
        ")\n",
        "\n",
        "# kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\n",
        "# # Create out of folds array\n",
        "# oof_predictions = np.zeros((X.shape[0], 1))\n",
        "# test_predictions = np.zeros(X_test.shape[0])\n",
        "# feature_importances = pd.DataFrame()\n",
        "# feature_importances[\"feature\"] = X.columns.tolist()\n",
        "# stats = pd.DataFrame()\n",
        "# explain_matrices = []\n",
        "# masks_ =[]\n",
        "\n",
        "clf =  TabNetRegressor(**tabnet_params)\n",
        "\n",
        "# for fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\n",
        "#     print(f'Training fold {fold + 1}')\n",
        "#     X_train, X_val = X.iloc[trn_ind].values, X.iloc[val_ind].values\n",
        "#     y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n",
        "\n",
        "\n",
        "    \n",
        "#     clf.fit(\n",
        "#       X_train, y_train,\n",
        "#       eval_set=[(X_val, y_val)],\n",
        "#       max_epochs = 200,\n",
        "#       patience = 50,\n",
        "#       batch_size = 1024*20, \n",
        "#       virtual_batch_size = 128*20,\n",
        "#       num_workers = 4,\n",
        "#       drop_last = False,\n",
        "#       eval_metric=[RMSPE],\n",
        "#       loss_fn=RMSPELoss\n",
        "#       )\n",
        "    \n",
        "#     saving_path_name = f\"./fold{fold}\"\n",
        "#     saved_filepath = clf.save_model(saving_path_name)\n",
        "    \n",
        "#     explain_matrix, masks = clf.explain(X_val)\n",
        "#     explain_matrices.append(explain_matrix)\n",
        "#     masks_.append(masks[0])\n",
        "#     masks_.append(masks[1])\n",
        "      \n",
        "#     oof_predictions[val_ind] = clf.predict(X_val)\n",
        "#     test_predictions+=clf.predict(X_test.values).flatten()/5\n",
        "#     feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n",
        "    \n",
        "#     stats[f'fold{fold+1}_train_rmspe']=clf.history['loss']\n",
        "#     stats[f'fold{fold+1}_val_rmspe']=clf.history['val_0_rmspe']\n",
        "preds=[]\n",
        "for path in modelpath:\n",
        "    \n",
        "    clf.load_model(path)\n",
        "    preds.append(clf.predict(X_test.values).squeeze(-1))\n",
        "    \n",
        "model1_predictions = np.mean(preds,axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REF4-_cv89DR",
        "outputId": "25d78261-894b-4163-8cc6-d23553a88bea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LGBM+NN"
      ],
      "metadata": {
        "id": "Y4u8ymwl9D_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import numpy.matlib\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qHDWuhxc9E1l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
        "    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n",
        "    labels_num = np.max(y) + 1\n",
        "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
        "    y_distr = Counter()\n",
        "    for label, g in zip(y, groups):\n",
        "        y_counts_per_group[g][label] += 1\n",
        "        y_distr[label] += 1\n",
        "\n",
        "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
        "    groups_per_fold = defaultdict(set)\n",
        "\n",
        "    def eval_y_counts_per_fold(y_counts, fold):\n",
        "        y_counts_per_fold[fold] += y_counts\n",
        "        std_per_label = []\n",
        "        for label in range(labels_num):\n",
        "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
        "            std_per_label.append(label_std)\n",
        "        y_counts_per_fold[fold] -= y_counts\n",
        "        return np.mean(std_per_label)\n",
        "    \n",
        "    groups_and_y_counts = list(y_counts_per_group.items())\n",
        "    random.Random(seed).shuffle(groups_and_y_counts)\n",
        "\n",
        "    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n",
        "        best_fold = None\n",
        "        min_eval = None\n",
        "        for i in range(k):\n",
        "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
        "            if min_eval is None or fold_eval < min_eval:\n",
        "                min_eval = fold_eval\n",
        "                best_fold = i\n",
        "        y_counts_per_fold[best_fold] += y_counts\n",
        "        groups_per_fold[best_fold].add(g)\n",
        "\n",
        "    all_groups = set(groups)\n",
        "    for i in range(k):\n",
        "        train_groups = all_groups - groups_per_fold[i]\n",
        "        test_groups = groups_per_fold[i]\n",
        "\n",
        "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
        "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
        "\n",
        "        yield train_indices, test_indices"
      ],
      "metadata": {
        "id": "A9PZvpjJ9IfV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_train_test():\n",
        "    # Function to read our base train and test set\n",
        "    \n",
        "    train = pd.read_csv('/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/train.csv')\n",
        "    test = pd.read_csv('/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/test.csv')\n",
        "\n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    print(f'Our test set has {test.shape[0]} rows')\n",
        "    print(f'Our training set has {train.isna().sum().sum()} missing values')\n",
        "    print(f'Our test set has {test.isna().sum().sum()} missing values')\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "train, test = read_train_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPCUSbrp9JYj",
        "outputId": "a6f1787f-f810-479c-bf8a-f20435e6c9a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our training set has 428932 rows\n",
            "Our test set has 3 rows\n",
            "Our training set has 0 missing values\n",
            "Our test set has 0 missing values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data directory\n",
        "data_dir = '/content/drive/MyDrive/22-여름 스터디/Optiver Realized Volatility Prediction/'\n",
        "\n",
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "\n",
        "# Function to aggregate 1st and 2nd WAP\n",
        "def calc_wap12(df):\n",
        "    var1 = df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']\n",
        "    var2 = df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']\n",
        "    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n",
        "    return (var1+var2) / den\n",
        "\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "def calc_wap34(df):\n",
        "    var1 = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']\n",
        "    var2 = df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
        "    den = df['bid_size1'] + df['ask_size1'] + df['bid_size2'] + df['ask_size2']\n",
        "    return (var1+var2) / den\n",
        "\n",
        "def calc_swap12(df):\n",
        "    return df['wap12'] - df['wap34']\n",
        "\n",
        "def calc_tswap1(df):\n",
        "    return -df['swap1'].diff()\n",
        "\n",
        "def calc_tswap12(df):\n",
        "    return -df['swap12'].diff()\n",
        "\n",
        "def calc_wss12(df):\n",
        "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
        "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
        "    return (ask - bid) / df['midprice']"
      ],
      "metadata": {
        "id": "X2V70Pbk9LR9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate order book depth\n",
        "def calc_depth(df):\n",
        "    depth = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1'] + df['bid_price2'] * df[\n",
        "               'bid_size2'] + df['ask_price2'] * df['ask_size2']\n",
        "    return depth\n",
        "\n",
        "# Calculate order book slope\n",
        "def calc_slope(df):\n",
        "    v0 = (df['bid_size1']+df['ask_size1'])/2\n",
        "    p0 = (df['bid_price1']+df['ask_price1'])/2\n",
        "    slope_bid = ((df['bid_size1']/v0)-1)/abs((df['bid_price1']/p0)-1)+(\n",
        "                (df['bid_size2']/df['bid_size1'])-1)/abs((df['bid_price2']/df['bid_price1'])-1)\n",
        "    slope_ask = ((df['ask_size1']/v0)-1)/abs((df['ask_price1']/p0)-1)+(\n",
        "                (df['ask_size2']/df['ask_size1'])-1)/abs((df['ask_price2']/df['ask_price1'])-1)\n",
        "    return (slope_bid+slope_ask)/2, abs(slope_bid-slope_ask)\n",
        "\n",
        "# Calculate order book dispersion\n",
        "def calc_dispersion(df):\n",
        "    bspread = df['bid_price1'] - df['bid_price2']\n",
        "    aspread = df['ask_price2'] - df['ask_price1']\n",
        "    bmid = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price1']\n",
        "    bmid2 = (df['bid_price1'] + df['ask_price1'])/2  - df['bid_price2']\n",
        "    amid = df['ask_price1'] - (df['bid_price1'] + df['ask_price1'])/2\n",
        "    amid2 = df['ask_price2'] - (df['bid_price1'] + df['ask_price1'])/2\n",
        "    bdisp = (df['bid_size1']*bmid + df['bid_size2']*bspread)/(df['bid_size1']+df['bid_size2'])\n",
        "    bdisp2 = (df['bid_size1']*bmid + df['bid_size2']*bmid2)/(df['bid_size1']+df['bid_size2'])\n",
        "    adisp = (df['ask_size1']*amid + df['ask_size2']*aspread)/(df['ask_size1']+df['ask_size2'])      \n",
        "    adisp2 = (df['ask_size1']*amid + df['ask_size2']*amid2)/(df['ask_size1']+df['ask_size2'])\n",
        "    return (bdisp + adisp)/2, (bdisp2 + adisp2)/2\n",
        "\n",
        "def calc_price_impact(df):\n",
        "    ask = (df['ask_price1'] * df['ask_size1'] + df['ask_price2'] * df['ask_size2'])/(df['ask_size1']+df['ask_size2'])\n",
        "    bid = (df['bid_price1'] * df['bid_size1'] + df['bid_price2'] * df['bid_size2'])/(df['bid_size1']+df['bid_size2'])\n",
        "    return (df['ask_price1'] - ask)/df['ask_price1'], (df['bid_price1'] - bid)/df['bid_price1']\n",
        "\n",
        "#  order flow imbalance\n",
        "def calc_ofi(df):\n",
        "    a = df['bid_size1']*np.where(df['bid_price1'].diff()>=0,1,0)\n",
        "    b = df['bid_size1'].shift()*np.where(df['bid_price1'].diff()<=0,1,0)\n",
        "    c = df['ask_size1']*np.where(df['ask_price1'].diff()<=0,1,0)\n",
        "    d = df['ask_size1'].shift()*np.where(df['ask_price1'].diff()>=0,1,0)\n",
        "    return a - b - c + d\n",
        "\n",
        "# Turnover\n",
        "def calc_tt1(df):\n",
        "    p1 = df['ask_price1'] * df['ask_size1'] + df['bid_price1'] * df['bid_size1']\n",
        "    p2 = df['ask_price2'] * df['ask_size2'] + df['bid_price2'] * df['bid_size2']      \n",
        "    return p2 - p1 \n",
        "\n",
        "# Function to calculate the log of the return\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "def log_return_out(series):\n",
        "    ret = np.log(series).diff()\n",
        "    return remove_outliers(ret)\n",
        "\n",
        "def remove_outliers(series):\n",
        "    cu = 6\n",
        "    ser_mean, ser_std = np.mean(series), np.std(series)\n",
        "    series = series.where(series<=(ser_mean + cu*ser_std), ser_mean)\n",
        "    series = series.where(series>=(ser_mean - cu*ser_std), ser_mean)\n",
        "    return series\n",
        "\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "    \n",
        "def realized_volatility_downside(series):\n",
        "    return np.sqrt(np.sum(series[series<0]**2))\n",
        "\n",
        "def realized_volatility_upside(series):\n",
        "    return np.sqrt(np.sum(series[series>0]**2))\n",
        "\n",
        "# realized bipower variation \n",
        "def realized_bipowvar(series):\n",
        "    cnt = series.count()\n",
        "    if cnt<3:\n",
        "        return np.nan\n",
        "    else:\n",
        "        cons = (np.pi/2)*(cnt/(cnt-2))\n",
        "        return cons*np.nansum(np.abs(series)*np.abs(series.shift()))\n",
        "    \n",
        "# Calculate integrated quarticity\n",
        "def realized_quarticity(series):\n",
        "    return (series.count()/3)*np.sum(series**4)\n",
        "\n",
        "# realized median variation \n",
        "def realized_medianvar(series):\n",
        "    cnt = series.count()\n",
        "    if cnt<3:\n",
        "        return np.nan\n",
        "    else:\n",
        "        cons = (np.pi/(6-4*np.sqrt(3)+np.pi))*(cnt/(cnt-2))\n",
        "        return cons*np.nansum(np.median([np.abs(series),np.abs(series.shift()),np.abs(series.shift(2))], axis=0)**2)\n",
        "    \n",
        "# Calculate the realized absolute variatian\n",
        "def realized_absvar(series):\n",
        "    return np.sqrt(np.pi/(2*series.count()))*np.sum(np.abs(series))\n",
        "\n",
        "# Calculate weighted volatility\n",
        "def realized_vol_weighted(series):\n",
        "    return np.sqrt(np.sum(series**2)/series.count())\n",
        "\n",
        "# Calculate the realized skew\n",
        "def realized_skew(series):\n",
        "    return np.sqrt(series.count())*np.sum(series**3)/(realized_volatility(series)**3)\n",
        "\n",
        "# Calculate the realized kurtosis\n",
        "def realized_kurtosis(series):\n",
        "    return series.count()*np.sum(series**4)/(realized_volatility(series)**4)"
      ],
      "metadata": {
        "id": "L34lYDxO9T89"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get group stats for different windows (seconds in bucket)\n",
        "def get_stats_bins(df, feat_dict, bins, quantile=False):\n",
        "    # Group by the window\n",
        "    if bins==0:\n",
        "        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n",
        "    else:\n",
        "        if quantile:\n",
        "            df['sbins'] = pd.qcut(df.seconds_in_bucket, bins, labels=False)\n",
        "            q = 'q'\n",
        "        else:\n",
        "            df['sbins'] = pd.cut(df.seconds_in_bucket, bins, labels=False)\n",
        "            q = ''\n",
        "        df_feature = None\n",
        "        for i in range(bins):\n",
        "            df_feat = df.loc[df.sbins==i].groupby('time_id').agg(feat_dict).reset_index()\n",
        "            # Rename columns joining suffix\n",
        "            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
        "            # Add a suffix to differentiate bins\n",
        "            df_feat = df_feat.rename(columns={col: col+'_'+q+str(bins)+'bins_'+str(i) for col in df_feat.columns})\n",
        "            df_feat = df_feat.rename(columns={'time_id_'+'_'+q+str(bins)+'bins_'+str(i): 'time_id'})\n",
        "            if isinstance(df_feature, pd.DataFrame):\n",
        "                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n",
        "            else:\n",
        "                df_feature = df_feat.copy()\n",
        "        df = df.drop('sbins', axis=1)\n",
        "    return df_feature"
      ],
      "metadata": {
        "id": "FHI2ReMb9d3m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get group stats for different windows (seconds in bucket)\n",
        "def get_stats_window(df, feat_dict, window):\n",
        "    # Group by the window\n",
        "    if window==0:\n",
        "        df_feature = df.groupby('time_id').agg(feat_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        df_feature = df_feature.rename(columns={'time_id_': 'time_id'})\n",
        "    else:\n",
        "        df_feature = None\n",
        "        for w in range(window, 600, window):\n",
        "            df_feat = df.loc[df.seconds_in_bucket>=w].groupby('time_id').agg(feat_dict).reset_index()\n",
        "            # Rename columns joining suffix\n",
        "            df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
        "            # Add a suffix to differentiate bins\n",
        "            df_feat = df_feat.rename(columns={col: col+'_'+str(window)+'win_'+str(w) for col in df_feat.columns})\n",
        "            df_feat = df_feat.rename(columns={'time_id_'+'_'+str(window)+'win_'+str(w): 'time_id'})\n",
        "            if isinstance(df_feature, pd.DataFrame):\n",
        "                df_feature = pd.merge(df_feature, df_feat, how='left', on='time_id')\n",
        "            else:\n",
        "                df_feature = df_feat.copy()\n",
        "    return df_feature"
      ],
      "metadata": {
        "id": "20D03i2J9f3N"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample from the window and average\n",
        "def get_stats_sampled(df, feat_dict, numsamp, fraction, boot):\n",
        "    df_feature = None\n",
        "    for i in range(numsamp):\n",
        "        df_feat = df.groupby('time_id').sample(frac=fraction, random_state=i, replace=boot).reset_index(drop=True)\n",
        "        df_feat = df_feat.groupby('time_id').agg(feat_dict).reset_index()\n",
        "        df_feat.columns = ['_'.join(col) for col in df_feat.columns]\n",
        "        if isinstance(df_feature, pd.DataFrame):\n",
        "            df_feature += df_feat.values/numsamp   \n",
        "        else:\n",
        "            df_feature = df_feat.copy()/numsamp\n",
        "    df_feature = df_feature.rename(columns={col: col+'_sample_'+str(fraction)+'_'+str(numsamp) for col in df_feature.columns})\n",
        "    df_feature = df_feature.rename(columns={'time_id_'+'_sample_'+str(fraction)+'_'+str(numsamp): 'time_id'})\n",
        "    return df_feature"
      ],
      "metadata": {
        "id": "qiJSfbJc9hm9"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    #### Data Cleansing (if any needed)\n",
        "    \n",
        "    df = df.drop(df.loc[df.ask_price1 <= 0].index)\n",
        "    df = df.drop(df.loc[df.bid_price1 <= 0].index)\n",
        "    df = df.drop(df.loc[(df['ask_price1'] - df['bid_price1']) < 0].index)\n",
        "    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n",
        "    \n",
        "    ####\n",
        "    \n",
        "    # Calculate prices\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    \n",
        "    df['depth'] = calc_depth(df)\n",
        "    df['slope'], _ = calc_slope(df)\n",
        "\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n",
        "    \n",
        "    # Calculate spreads\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['dispersion'], _ = calc_dispersion(df)\n",
        "\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'depth': [np.sum],\n",
        "        'slope': [np.sum],\n",
        "        'log_return1': [realized_volatility, realized_absvar],\n",
        "        'log_return2': [realized_volatility, realized_absvar],\n",
        "        'price_spread': [np.sum, np.nanmedian],\n",
        "    }\n",
        "\n",
        "    create_feature_dict_bins = {\n",
        "        'depth': [np.sum],\n",
        "        'slope': [np.sum],\n",
        "        'dispersion': [np.sum],\n",
        "        'log_return1': [realized_volatility, realized_absvar],\n",
        "        'log_return2': [realized_volatility, realized_absvar],\n",
        "        'price_spread': [np.sum],\n",
        "    }\n",
        "\n",
        "\n",
        "    # Get the stats for different windows\n",
        "    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n",
        "    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n",
        "\n",
        "    df_feature_0 = df_feature_0.add_prefix('book_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature_0['row_id'] = df_feature_0['book_time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature_0.drop(['book_time_id'], axis = 1, inplace = True)\n",
        "    return df_feature_0"
      ],
      "metadata": {
        "id": "_pVXzNrM9jCc"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    #### Data Cleansing\n",
        "    \n",
        "    df = df.drop(df.loc[df.price <= 0].index)\n",
        "    df = df.groupby(['time_id','seconds_in_bucket']).mean().reset_index()\n",
        "    \n",
        "    ####\n",
        "    \n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return': [realized_volatility, realized_absvar],\n",
        "        'order_count':[np.sum, np.max],\n",
        "    }\n",
        "    \n",
        "    create_feature_dict_bins = {\n",
        "        'log_return': [realized_volatility, realized_absvar],\n",
        "        'order_count': [np.sum],\n",
        "    }\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature_0 = get_stats_bins(df, create_feature_dict, 0)\n",
        "    df_feature_w4 = get_stats_window(df, create_feature_dict_bins, 150)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature_0 = df_feature_0.merge(df_feature_w4, how = 'left', on = 'time_id')   \n",
        "\n",
        "    df_feature_0 = df_feature_0.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature_0['row_id'] = df_feature_0['trade_time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature_0.drop(['trade_time_id'], axis = 1, inplace = True)\n",
        "    return df_feature_0"
      ],
      "metadata": {
        "id": "ufzi9h0e9lqo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')      \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "MLmO7qd89oax"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD4oudaU9q9q",
        "outputId": "933429b9-bb87-4787-d01c-268a3837f7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['trade_size_tau'] = np.sqrt(1/train['trade_order_count_sum'])\n",
        "    \n",
        "for w in range(150, 600, 150):\n",
        "    train['trade_size_tau_150win_'+str(w)] = np.sqrt(1/train['trade_order_count_sum_150win_'+str(w)])"
      ],
      "metadata": {
        "id": "1ZIvI-l79som"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['trade_size_tau'] = np.sqrt(1/test['trade_order_count_sum'])\n",
        "    \n",
        "for w in range(150, 600, 150):\n",
        "    test['trade_size_tau_150win_'+str(w)] = np.sqrt(1/test['trade_order_count_sum_150win_'+str(w)])"
      ],
      "metadata": {
        "id": "JnJEXan_9uiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
        "\n",
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_agg(df):\n",
        "    gcols=['book_log_return1_realized_volatility']\n",
        "    gcols+=['book_log_return1_realized_volatility_150win_150']\n",
        "    gcols+=['book_log_return2_realized_volatility_150win_300']\n",
        "    gcols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\n",
        "    gcols+=['book_price_spread_sum_150win_150']\n",
        "    gcols+=['trade_size_tau_150win_150']\n",
        "    gcols+=['book_depth_sum_150win_150']\n",
        "    gcols+=['book_dispersion_sum_150win_150']\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby('time_id')[gcols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df_time_id = df_time_id.rename(columns={'time_id__time':'time_id'})\n",
        "    return df_time_id, [col for col in df_time_id if col not in ['time_id']]"
      ],
      "metadata": {
        "id": "zHCP4QN29wg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['row_id','time_id','stock_id','target']\n",
        "cols+=['book_log_return1_realized_volatility']\n",
        "cols+=['book_log_return1_realized_volatility_150win_150']+['book_log_return2_realized_volatility_150win_150']+[\n",
        "            'trade_log_return_realized_volatility_150win_150']\n",
        "cols+=['book_log_return1_realized_absvar_150win_150']+['book_log_return2_realized_absvar_150win_150']+[\n",
        "            'trade_log_return_realized_absvar_150win_150']\n",
        "cols+=['book_log_return2_realized_volatility_150win_300']\n",
        "cols+=['book_log_return1_realized_volatility_150win_450']+['trade_log_return_realized_volatility_150win_450']\n",
        "cols+=['book_price_spread_sum_150win_150']\n",
        "cols+=['trade_size_tau_150win_150']\n",
        "cols+=['book_depth_sum_150win_150']\n",
        "cols+=['book_dispersion_sum_150win_150']\n",
        "\n",
        "train = train[[col for col in train.columns if col in cols]]\n",
        "test = test[[col for col in test.columns if col in cols]]"
      ],
      "metadata": {
        "id": "1Tj3lttv9xGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed0=2021\n",
        "params0 = {\n",
        "    'objective': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_depth': 4,\n",
        "    'num_leaves': 15,\n",
        "    'min_data_in_leaf': 250,\n",
        "    'learning_rate': 0.1,\n",
        "    'subsample': 0.9,\n",
        "    'subsample_freq': 1,\n",
        "    'feature_fraction': 0.8,\n",
        "    'categorical_column': [0],\n",
        "    'seed':seed0,\n",
        "    'feature_fraction_seed': seed0,\n",
        "    'bagging_seed': seed0,\n",
        "    'drop_seed': seed0,\n",
        "    'data_random_seed': seed0,\n",
        "    'n_jobs':-1,\n",
        "    'verbose': -1}\n",
        "\n",
        "def train_and_evaluate_lgb(train, test, params, try_seed=seed0):\n",
        "    # Hyperparammeters (just basic)\n",
        "\n",
        "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "    feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n",
        "    # Create out of folds array\n",
        "    y = train['target']\n",
        "    oof_predictions = np.zeros(train.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kf = 5\n",
        "    # Iterate through each fold\n",
        "    skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n",
        "                                  groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=try_seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(skf):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
        "        print(x_train.shape)\n",
        "        print(x_val.shape)\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        tt=test.copy()\n",
        "\n",
        "        x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n",
        "        x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n",
        "        x_val_agg_time, _ = get_time_agg(x_val)\n",
        "        x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n",
        "        test_agg_time, _ = get_time_agg(tt)\n",
        "        tt = tt.merge(test_agg_time, how='left', on='time_id')\n",
        "        del x_train_agg_time,  x_val_agg_time, test_agg_time\n",
        "        gc.collect()\n",
        "        \n",
        "        traincols = features+agg_feats_time\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train[traincols], y_train, weight = train_weights)\n",
        "        val_dataset = lgb.Dataset(x_val[traincols], y_val, weight = val_weights)\n",
        "        model = lgb.train(params = params,\n",
        "                          num_boost_round=1000,\n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          verbose_eval = 50,\n",
        "                          early_stopping_rounds=50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val[traincols])\n",
        "        # Predict the test set       \n",
        "        test_predictions += model.predict(tt[traincols]) / kf\n",
        "        plt.rcParams[\"figure.figsize\"] = (14, 7) \n",
        "        lgb.plot_importance(model, max_num_features=35)\n",
        "        plt.rcParams[\"figure.figsize\"] = (14, 7) \n",
        "        lgb.plot_importance(model, max_num_features=35, importance_type='gain')\n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "\n",
        "    # Return test predictions\n",
        "    return test_predictions\n",
        "\n",
        "# Traing and evaluate\n",
        "predictions_lgb_1 = train_and_evaluate_lgb(train, test, params0)"
      ],
      "metadata": {
        "id": "L8kP9ler9zmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed0=2021\n",
        "params0 = {\n",
        "    'objective': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_depth': 5,\n",
        "    'num_leaves': 31,\n",
        "    'min_data_in_leaf': 100,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.95,\n",
        "    'subsample_freq': 1,\n",
        "    'feature_fraction': 0.9,\n",
        "    'categorical_column': [0],\n",
        "    'seed':seed0,\n",
        "    'feature_fraction_seed': seed0,\n",
        "    'bagging_seed': seed0,\n",
        "    'drop_seed': seed0,\n",
        "    'data_random_seed': seed0,\n",
        "    'n_jobs':-1,\n",
        "    'verbose': -1}\n",
        "\n",
        "def train_and_evaluate_lgb(train, test, params, try_seed=seed0):\n",
        "    # Hyperparammeters (just basic)\n",
        "\n",
        "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "    feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n",
        "    # Create out of folds array\n",
        "    y = train['target']\n",
        "    oof_predictions = np.zeros(train.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kf = 10\n",
        "    # Iterate through each fold\n",
        "    skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n",
        "                                  groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=try_seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(skf):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
        "        print(x_train.shape)\n",
        "        print(x_val.shape)\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        tt=test.copy()\n",
        "\n",
        "        x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n",
        "        x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n",
        "        x_val_agg_time, _ = get_time_agg(x_val)\n",
        "        x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n",
        "        test_agg_time, _ = get_time_agg(tt)\n",
        "        tt = tt.merge(test_agg_time, how='left', on='time_id')\n",
        "        del x_train_agg_time,  x_val_agg_time, test_agg_time\n",
        "        gc.collect()\n",
        "        \n",
        "        traincols = features+agg_feats_time\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train[traincols], y_train, weight = train_weights)\n",
        "        val_dataset = lgb.Dataset(x_val[traincols], y_val, weight = val_weights)\n",
        "        model = lgb.train(params = params,\n",
        "                          num_boost_round=1000,\n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          verbose_eval = 50,\n",
        "                          early_stopping_rounds=50,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val[traincols])\n",
        "        # Predict the test set       \n",
        "        test_predictions += model.predict(tt[traincols]) / kf\n",
        "        plt.rcParams[\"figure.figsize\"] = (14, 7) \n",
        "        lgb.plot_importance(model, max_num_features=35)\n",
        "        plt.rcParams[\"figure.figsize\"] = (14, 7) \n",
        "        lgb.plot_importance(model, max_num_features=35, importance_type='gain')\n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "\n",
        "    # Return test predictions\n",
        "    return test_predictions\n",
        "\n",
        "# Traing and evaluate\n",
        "predictions_lgb_2 = train_and_evaluate_lgb(train, test, params0)"
      ],
      "metadata": {
        "id": "CUDP903a90Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.random import seed\n",
        "import tensorflow as tf\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.backend import sigmoid\n",
        "\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "\n",
        "get_custom_objects().update({'swish': Activation(swish)})\n",
        "\n",
        "def root_mean_squared_per_error(y_true, y_pred):\n",
        "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))"
      ],
      "metadata": {
        "id": "DmM9QN9G98Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def base_model(numfeats, cat_data=train['stock_id']):\n",
        "    \n",
        "    hidden_units = (64, 32, 32, 16, 16, 16, 16, 8, 8)\n",
        "    stock_embedding_size = 24\n",
        "    \n",
        "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
        "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
        "    num_input = keras.Input(shape=(numfeats,), name='num_data')\n",
        "\n",
        "\n",
        "    #embedding, flatenning and concatenating\n",
        "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
        "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
        "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
        "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
        "    \n",
        "    # Add one or more hidden layers\n",
        "    for n_hidden in hidden_units:\n",
        "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
        "\n",
        "    #out = keras.layers.Concatenate()([out, num_input])\n",
        "\n",
        "    # A single output: our predicted rating\n",
        "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
        "    \n",
        "    model = keras.Model(\n",
        "    inputs = [stock_id_input, num_input],\n",
        "    outputs = out,\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "tA_m1yqo-BZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "seed(42)\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose=0,\n",
        "                                      mode='min', restore_best_weights=True)\n",
        "\n",
        "plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, \n",
        "                                               verbose=0, mode='min')\n",
        "\n",
        "scores_folds = {}\n",
        "model_name = 'NN'\n",
        "\n",
        "scores_folds[model_name] = []\n",
        "counter = 1\n",
        "\n",
        "predictions_nn_1 = np.zeros(test.shape[0])\n",
        "y = train['target']\n",
        "# Create a KFold object\n",
        "kf = 5\n",
        "# Iterate through each fold\n",
        "features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "feats_nostock = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] \n",
        "skf = stratified_group_k_fold(X=train[feats_nostock], y=train['stock_id'].astype('category').cat.codes.values, \n",
        "                              groups=np.array(train['time_id'].astype('category').cat.codes.values), k=kf, seed=2021)\n",
        "for fold, (trn_ind, val_ind) in enumerate(skf):\n",
        "    print(f'Training fold {fold + 1}')\n",
        "    \n",
        "    x_train = train.iloc[trn_ind].copy()\n",
        "    x_val = train.iloc[val_ind].copy()\n",
        "    print(x_train.shape)\n",
        "    print(x_val.shape)\n",
        "    y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "    tt = test.copy()\n",
        "\n",
        "    x_train.loc[:, feats_nostock] = x_train.loc[:, feats_nostock].fillna(x_train.groupby('stock_id')[feats_nostock].transform('median')).values\n",
        "    for i in x_val.stock_id.unique():\n",
        "        x_val.loc[x_val.stock_id==i, feats_nostock] = x_val.loc[x_val.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n",
        "    for i in tt.stock_id.unique():\n",
        "        tt.loc[tt.stock_id==i, feats_nostock] = tt.loc[tt.stock_id==i, feats_nostock].fillna(x_train.loc[x_train.stock_id==i, feats_nostock].median()).values\n",
        "    \n",
        "    x_train_agg_time, agg_feats_time = get_time_agg(x_train)\n",
        "    x_train = x_train.merge(x_train_agg_time, how='left', on='time_id')\n",
        "    x_val_agg_time, _ = get_time_agg(x_val)\n",
        "    x_val = x_val.merge(x_val_agg_time, how='left', on='time_id')\n",
        "    test_agg_time, _ = get_time_agg(tt)\n",
        "    tt = tt.merge(test_agg_time, how='left', on='time_id')\n",
        "    del x_train_agg_time,  x_val_agg_time, test_agg_time\n",
        "    gc.collect()\n",
        "\n",
        "    traincols = feats_nostock+agg_feats_time\n",
        "\n",
        "    for i in tt.stock_id.unique():\n",
        "        tt.loc[tt.stock_id==i, traincols] = tt.loc[tt.stock_id==i, traincols].fillna(x_train.loc[x_train.stock_id==i, traincols].median()).values\n",
        "\n",
        "    num_trans = Pipeline([('qt', QuantileTransformer(n_quantiles=2000, output_distribution='normal')),\n",
        "                         ('numscaler', MinMaxScaler())])\n",
        "    agg_trans = Pipeline([('aggscaler', MinMaxScaler())])\n",
        "    preprocessor = ColumnTransformer(transformers=[('num', num_trans, feats_nostock),\n",
        "                                                    ('agg', agg_trans, agg_feats_time)])\n",
        "    pipe = Pipeline([('pp', preprocessor)])\n",
        "    pipe.fit(x_train[traincols])\n",
        "    \n",
        "    model = base_model(len(traincols))\n",
        "    model.compile(\n",
        "        keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=root_mean_squared_per_error\n",
        "    )\n",
        "    \n",
        "    model.fit([x_train['stock_id'], pipe.transform(x_train[traincols])], \n",
        "              y_train,               \n",
        "              batch_size=2048,\n",
        "              epochs=1000,\n",
        "              validation_data=([x_val['stock_id'], pipe.transform(x_val[traincols])], y_val),\n",
        "              callbacks=[es, plateau],\n",
        "              validation_batch_size=len(y_val),\n",
        "              shuffle=True,\n",
        "              verbose = 1)\n",
        "\n",
        "    preds = model.predict([x_val['stock_id'], pipe.transform(x_val[traincols])]).reshape(1,-1)[0]\n",
        "    \n",
        "    score = round(rmspe(y_true = y_val, y_pred = preds), 5)\n",
        "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
        "    scores_folds[model_name].append(score)\n",
        "    \n",
        "    predictions_nn_1 += model.predict([tt['stock_id'], pipe.transform(tt[traincols])]).reshape(1,-1)[0].clip(0,1e10) / kf\n",
        "\n",
        "    counter += 1\n",
        "    \n",
        "print('RMSPE {}: Folds: {}'.format(model_name, scores_folds[model_name]))"
      ],
      "metadata": {
        "id": "zIyH4Gzy-MvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['target'] = 0.988*predictions_nn_1*0.3+model1_predictions*0.3 + predictions_lgb_1*0.2 + predictions_lgb_2*0.2\n",
        "\n",
        "display(test[['row_id', 'target']].head(3))\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "id": "lECmG7EE-hVV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}